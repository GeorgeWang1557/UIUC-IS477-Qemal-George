The original project plan set out to study global earthquake activity and compare it to long-term changes in Earth’s surface temperature, using two large datasets from Kaggle. The goal was never to prove causation but to look for patterns over time and across regions. The early project work focused on downloading the datasets, cleaning them, and preparing them for analysis. Since then, the project has moved forward in a pretty significant way. The biggest change is that everything has now been automated through Python. Instead of downloading files manually, the datasets load directly through the KaggleHub API, which gives us full reproducibility. Integrity checks using SHA-256 hashing are also built into the workflow, which helps verify that the data we are analyzing is the same every time. This shift in approach has made the project cleaner, more reliable, and easier to maintain.
On the data-acquisition side, the entire process is now programmed. The earthquake and climate datasets are downloaded automatically through kagglehub.dataset_download(). This was originally going to be a manual step, but writing code to automate it makes the workflow much stronger. The repo now includes Python scripts that pull the data, preview it, verify file integrity, and prepare it for cleaning. Both datasets load correctly, and the hashing results confirm that the files remain consistent. This phase of the project is fully complete, and all of the relevant files, screenshots, and paths are documented in the GitHub repository.
The analysis portion of the project has changed in a meaningful way from the original plan. Initially, Tableau was supposed to be used for the visualizations. Instead, the entire analysis has shifted into Python. This was done because Python is easier to integrate with the cleaning and merging steps, and it also supports reproducibility, which is a major requirement of the course. So far, the analysis includes extracting year fields, converting coordinate formats, handling missing values, and aggregating both datasets by year. The earthquake dataset has been summarized by yearly counts, average magnitude, average depth, and tsunami occurrences. The climate data has been summarized by average yearly temperature and uncertainty. These features are then merged into a single dataset that allows for direct comparisons over time.
Beyond the global integration, the project now includes regional analysis through latitude and longitude binning. This means that earthquake locations are grouped into 10-degree latitude and longitude ranges, creating regional summaries that can be compared to temperature trends in similar regions. This step was not in the original project plan, but adding it helped make the analysis more meaningful. It gives the project spatial context, not just global averages. It also addresses one of the original constraints about geographic accuracy. Instead of trying to match countries perfectly, we standardized coordinates and built a region model that is simple, clean, and effective.
The integration scripts in the repository now handle both global and regional merging. The global merged dataset contains yearly summaries, while the regional dataset breaks activity into geographic bins. Both of these files are saved in the processed data folder, and the scripts that create them are all tracked. The project’s organization is much stronger than before, and the structure now reflects a proper end-to-end workflow. There are separate scripts for loading data, cleaning, integrating, analyzing, and running SQL queries through DuckDB. These steps go beyond what the project originally required, but they make the final report easier to write and the workflow easier to justify.
The timeline from the original plan is mostly on schedule. Team selection and the project plan have already been completed. The interim status report is being submitted on time. The remaining tasks mostly involve polishing the analysis, generating finalized visualizations, running additional statistical tests, and preparing the final documentation. A more detailed timeline has now been created. Visualization work should be finished by late November. SQL analysis through DuckDB will be completed shortly afterward. A RunAll script or Snakemake workflow will be added around late November to automate the entire pipeline. The data dictionary, metadata files, and Box uploads will be completed before the end of November. The final project polishing and write-up will happen in early December, leading up to the final GitHub release.
The original project plan listed a few constraints and gaps that needed to be addressed. Most of these have now been handled. Geographic accuracy was solved using latitude and longitude binning instead of country-level matching. Statistical testing is now included through correlation analysis, and regression models may be added later depending on time. The idea of adding a third dataset is no longer necessary because the current datasets already provide enough material for a strong analysis. Visualization clarity is being handled through Python rather than Tableau, which improves reproducibility. Automation and reproducibility, which were important concerns in the initial plan, are now core features of the project thanks to the Python-based pipeline and the planned Snakemake/RunAll script.
Several changes have been made compared to the original plan, all of which strengthen the project. The switch from Tableau to Python makes everything cleaner and easier to repeat. Adding SHA-256 hashing ensures data integrity. Implementing regional analysis provides a deeper look at spatial patterns. Using DuckDB for SQL queries allows for faster, clearer exploration of the data. The cleaning work is also more advanced than originally expected, including coordinate transformations, datetime fixes, and aggregation. Finally, the decision to focus on automation meets the course’s requirement for reproducible workflows.
Both team members have contributed in different ways. George originally found the datasets and prepared the early research. He helped define the structure of the dataset merge and reviewed the sources. Since then, Qemal has handled the end-to-end technical pipeline, including the data-acquisition scripts, cleaning, coordinate standardization, merging, enrichment, correlation analysis, visualization, SQL queries, and documentation. A more formal summary from each team member will be committed separately to meet the milestone requirement. This will show clear contribution history in the GitHub commit log.
The GitHub repository now contains a well-structured layout with separate folders for source code, processed data, documentation, and upcoming workflow scripts. The /src folder includes code for loading data, cleaning, integrating, analyzing, and running SQL logic. The /data/processed folder contains the merged datasets. The /docs folder contains the project plan and will hold the status report. Additional files like the requirements.txt file and metadata docs will be added as the final submission approaches. This structure already makes the project more transparent and easier to grade.
Looking ahead, there are a few remaining tasks before the final submission. The project still needs polished visualizations for the report. The SQL analysis through DuckDB will be extended to include additional queries. The automation workflow will be implemented so the entire project can be re-run from start to finish with one command. The metadata, data dictionary, and Box uploads will be completed to meet the course’s reproducibility and FAIR requirements. Finally, the README will be expanded to describe each step in order, from data acquisition to final results. These tasks are all manageable within the remaining time, and the project is on track to finish strongly.
